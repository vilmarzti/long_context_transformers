data_loader: &data_loader
  samples: 
  valid_samples: 128
  batch_size: 8
  max_length: 64

train: &train
  epochs: 50
  subsequence_len: 64
  aggregate: 4

optimizer: 
  learning_rate: 0.0001

transformer_xl:
  n_layer: 8
  n_head: 8
  n_embed: 768
  d_inner: 3072 
  d_head: 218
  cutoffs: [2222, 4444, 22222]
  return_dict: True
  mem_len: 1
  train: 
    <<: *train
    subsequence_len: 32
  data_loader:
    <<: *data_loader
    max_length: 64
    samples: 

transformer_xl_tokenizer:
  pretrained_model_name_or_path: "data/tokenizer-xl-wiki2"
  verbose: False

compressive_transformer:
  n_layer: 4
  n_head: 8
  compression_rate: 3
  c_mem_length: 4
  mem_len: 2
  return_dict: True
  output_hidden_states: True
  cutoffs: [2222, 4444, 22222]
  train: 
    <<: *train
  data_loader: 
    <<: *data_loader

gpt:
  n_layer: 12
  train: 
    <<: *train
  data_loader: 
    <<: *data_loader

gpt_tokenizer:
  vocab_file: "data/tokenizer-gpt-wiki2/vocab.json"
  merges_file: "data/tokenizer-gpt-wiki2/merges.txt"
  unk_token: "<unk>"
